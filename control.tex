\section{Motion Control}
Humans and animals can effortlessly control their muscles in to achieve coordinated and purposeful motions, ranging from simple locomotion, dextrous hand manipulation, to highly skillful stunts. To model these motions in animation, simulating the passive dynamics is not enough. The key challenge that motion control tackles is that given a high level description of a motion task (e.g. walk at 5 m/s, grasp a bottle and open the cap, etc.), how much torques should be applied at each joint to successfully achieve the task while still keeps the overall motion natural and realistic. Motion control is the most extensively researched topic in physically-based character animation. The most commonly used technique is optimization. It searches for a controller to minimize an objective function subject to physical constraints. A controller is the character's ``algorithmic brain'' that decides how much joint torques ($\boldsymbol{\tau}$ in eq. \ref{} and \ref{}) are needed at different situations. A controller can be either \emph{feedforward}, \emph{feedback} or their combination. A feedforward (open-loop) controller is a trajectory of pre-computed joint toques, which does not rely on the current states. In contrast, a feedback (close-loop) controller computes the joint torques based on the current states. Most of the motions of humans and animals requires both feedforward and feedback control. For example, our walking gait roughly follows a predefined trajectory (feedforward) but we still need to constantly monitor and control our balance (feedback) based on the road condition, the walking speed and other possible perturbations. The objective function in the optimization is often hand crafted to achieve a high-level task and and to minimize energy expenditure. The constraints usually consist of dynamics equations, joint limits, torque limits, and other task-related requirements. In this section, we will discuss three established methods of motion control: space-time constraints, optimal control and reinforcement learning.

\subsection{Space Time Constraints}
\subsection{Optimal Control}
\subsection{Reinforcement Learning}

Reinforcement learning is motivated by the learning/training process of humans and animals. In a high level, reinforcement learning optimizes a controller by interacting with the physical environment with countless trials. If the controller produces a desired behavior to achieve a task, a reward is provided as the positive reinforcement. In contrast, if an undesired behavior is observed, no reward is offered. This reward system can gradually adapt the controller until eventually, optimal controllers for the task emerge. Reinforcement learning is an active research area, with numerous algorithms proposed every year. We refer readers to \cite{} for a thorough review. We will focus on policy search in remaining of this section. Policy search is a popular reinforcement learning algorithm in physically-based character animation. It performs extremely well in this field because it can tackle control problems with high-dimensional continuous state and action spaces, which is essential to control a human character. 

Mathematically, reinforcement learning formulates and solves a Markov Decision Process (MDP). An MDP is a tuple $(S, A, P_{sas'}, R, D, \gamma)$, where $S$ is the \emph{state} space; $A$ is the \emph{action} space; $P_{sas'}$ is the \emph{transition probability}; $R$ is the \emph{reward function}; $D$ is the distribution of the initial state $s_0 \in D$ and $\gamma \in [0, 1]$ is the discount factor of the reward over time. 

The states reflect what the current situation is and the actions are what a character can perform to achieve the specified task. Ideally, the state space should contain all the possible states of the articulated rigid body system, such as the joint angles $\vc{q}$, joint velocities $\dot{\vc{q}}$ and time $t$, and the actions should consist of all the joint torques $\vc{\tau}$. However, this means that the state and the action space can have hundreds of dimensions. Due to the \emph{curse of dimensionality}, solving an MDP in such a high dimensional continuous space is computational infeasible. In practice, researchers often carefully design states and actions specifically for the tasksin question to make the computation tractable. For example, if the task is to keep balance while standing. The state space need only include important features for balance, such as the center of mass (COM) and the center of the ground contact polygon. Similarly, given the well known balance strategy, such as ankel strategy \cite{} and hip strategy \cite{}, the action space can be as simple as a few torques at lower body joints. Using these prior knowledge of the task can greatly simplify the problem, which is a common practice in physically-based character animation. Transition probability $P_{sas'}$ output the probability that the next state is $s'$ if an action $a$ is taken at the current state $s$. In physically-based character animation, the transition probability is based dynamics. Given $s$ and $a$, a physical simulation can compute the next state $s'$. Although most of the physical simulations output a deterministic state instead of the probability. Random noise can be added in simulation to increase the robustness of the learned controller \cite{}.

A reward function evaluates how good the current state and action are. It is a mapping from the state-action space to a real number: $R: S\times A\mapsto \mathbb{R}$. Similar to Space-time constraints and optimal control, a reward function usually consists of two part, a task-related component that measures how far the current state is from the goal state and a component evaluates the naturalness of the motions. Designing a good reward function is essential to the success of the entire learning algorithm. A good design of the reward should be a smooth function that gives continuous positive reinforce whenever progress is made. Mathematically, this design will present gradient information that optimization solver can follow to maximize the rewards. In contrast, a common mistake is to give reward only when the task is achieved, which makes the reward function a narrow spike and flatly zero elsewhere. This should be avoided because most of the optimization algorithms would have trouble finding such a spike. For certain challenging tasks. manually designing a good reward function is tedious. Another option to extract a reward function from motion capture data through inverse Reinforcement learning \cite{}.

The solution of an MDP is a control \emph{policy} $\pi$ that maps the states space to the action space $\pi: S\mapsto A$. It determines what action to perform at each situation to fulfill the task. Note that the goal of MDP is not to maximize the short-term reward $R$ at the next state, but a long term value $V$. The \emph{return} of a policy is the accumulated rewards along the state trajectory starting at $s_0$ by following the policy $\pi$ for $N$ steps.
\begin{displaymath}
V^\pi(s_0)=\sum_{i=0}^N{\gamma^{N-i}R(s_i)}
\end{displaymath}
The reward at earlier states can be exponentially discounted over time. The \emph{value} of a policy is the expected return with respect to the random initial state $s_0$ drawn from D.
\begin{equation}
V(\pi)=E_{s_0\sim D}[V^\pi(s_0)]
\label{eq:policyValue}
\end{equation}
Using $V$ instead of $R$ as the optimization target prevents the controller from applying short-sighted greedy strategies. This agrees with our ability of long-term planning when we are executing our motions.

A policy can be an arbitrary function. A practical way to optimize the policy is to first parameterize it and then search for the optimal policy within a parameterized functional space $\pi^*\in\Pi$. Commonly-used parameterization include look-up tables, linear functions, splines and neural networks. Parameterization determines the potential quality of the optimal policy. The choice of parameterization is case by case. There is no consensus what the best practice is in terms of policy parameterization. Once the policy parameterization is decided, its parameters are optimized. First, one or more random policies are generated as an initial guess. These candidates are evaluated and improved iteratively until the optimization converges or a user-specified maximum number of iterations is reached.

To evaluate a policy, we can execute the policy in the simulation for N time steps, calculate the reward at each step and sum them up to get the value of the policy. Policy improvement is usually guided using the policy gradient \cite{Ng:2000:PPS}. However, reinforcement learning in character animation faces many unique challenges. In many character animation tasks, such as locomotion and hand manipulation, discrete contact events happens frequently due to establishing and breaking contact, which invalidates gradient and makes it difficult to apply continuous optimization solvers. For this reason, sample-based stochastic optimization techniques are widely used in physically-based character animation. Covariance Matrix Adaptation Evolution Strategy (CMA-ES) \cite{}, is the most ffrequent applied optimization methods for motion control. CMA can work as long as we can evaluate the value function. It does not need to evaluate gradient, does not rely on good initializations. More importantly, CMA is a ``global'' search algorithm that can explore multiple local minima. Although there is no guarantee that CMA will converge at the global minimum, in practice, it often finds good local minima in moderately high dimensional control spaces (\eg 20-30 dimensions). For the completeness of the chapter, we will briefly describe the CMA algorithm. Interested readers can refer to the original paper \cite{} for additional details.

CMA starts with an initial underlying Gaussian distribution in the policy parameter space. A population of samples are generated according to the distribution. Each CMA sample represents a control policy, which is evaluated through simulation. The CMA samples are sorted according to their values and a certain percent of the inferior samples are discarded at each iteration. The underlying Gaussian distribution is updated according to the remaining good samples and is used to generate the next generation of samples. This process is performed iteratively. Over iterations, the underlying distribution is shifted and narrowed. Eventually,  the distribution converges to a good region of the control space. The optimal controller is the  best CMA sample throughout the all iterations.

In summary, reinforcement learning is a generic method for motion control. It can automatically learn a wide range of behaviors through throusands of simulation trials. Reinforcement learning, more specifically, policy search, is becoming an increasingly popular approach to synthesize animations of the virtual characters. One good thing about reinforcement learning is that it does not assume the mathetmaical forms of the dynamics or the reward function. It treats the physical simulation as a black box, as long as the it can output the next state given the current state and action. Thus, it is not bounded to a particular dynamics model. The learning algorithm can be used even if the simulation software is upgraded. The main challenge of reinforcement learning is to design the state, the action space and the reward function. We need to put enough prior knowledge into the design so that the search space is small enough to be computationally feasible, but not too small that eliminates effective control policies. This requires a lot of manual tweaking and experience to make the algorithm work, especially for challenging motion tasks. 
