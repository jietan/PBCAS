\section{Motion Control}
Humans and animals can effortlessly control their muscles in to achieve coordinated and purposeful motions, ranging from simple locomotion, dextrous hand manipulation, to highly skillful stunts. To model these motions in animation, simulating the passive dynamics is not enough. The key challenge that motion control tackles is to find controllers that can achieve high level motion tasks (e.g. walk at 5 m/s, grasp a bottle and open the cap, etc.). In character animation, a controller is the character's ``algorithmic brain'' that decides how much torque ($\boldsymbol{\tau}$ in eq. \ref{} and \ref{}) are needed at each joint to successfully fulfil the task while still keeps the overall motion natural. Motion control is the most extensively researched topic in physically-based character animation. The most commonly used technique is through optimization. In this section, we will discuss two established methods of motion control: trajectory optimization and reinforcement learning.

\subsection{Trajectory Optimization}

Starting from the classical paper ``Spacetime Constraints'', trajectory optimization has become a mainstream technique inphysically-based character animtion. It searches for a controller that minimizes a cost function subject to physical constraints. The optimization has the following general form:
\begin{equation}
  \begin{array}{cl}
    \min_{\vc{x},\vc{u}}&\sum_{t=0}^{N}g(\vc{x}_t,\vc{u}_t)\\
    \textrm{subject to}&\\
    &\vc{x}_{t+1}=\vc{h}(\vc{x}_t)+\vc{B}_t\vc{u}_t
  \end{array}
  \label{eq:trajectoryOptimization}
\end{equation}
where $\vc{x}$ is the physical states, $\vc{u}$ is the control, $g$ is the cost function, which is often hand crafted to achieve the high-level task. For example, if the task is to walk at 5 m/s, a possible cost function could be the distance between the current COM of the character and a desired COM position that moves at 5 m/s. The constraint usually consists of the dynamics equations $\vc{h}$. Note that in character animation, the dynamics are usually nonlinear to the state $\vc{x}$ but linear to the control $\vc{u}$ (See eq. \ref{} and \ref{}). In addition, the constraints can also include joint limits, torque limits, and other task-related requirements.

To make it more concrete, we will revisit a simple example of controlling a single particle that is illustrated in the original ``Spacetime Constraints'' paper.
The task of this particle is to fly from point $\vc{a}$ to point $\vc{b}$ in $T$ second using a time-varying jet force $\vc{f}(t)$. The dynamics of the particle is $m\vc{\ddot{x}}-\vc{f}-m\vc{g}=0$, where $\vc{x}$ is its position, $m$ is its mass and $\vc{g}$ is gravity. The goal of this flight is to minimize the total fuel consumption $\int_0^T|\vc{f}|^2dt$. After discretization along time, the optimization has the following form:
\begin{displaymath}
  \begin{array}{cl}
    \min_{\vc{x},\vc{f}}&\sum_{t=0}^{N}|\vc{f}_t|^2\\
    \textrm{subject to}&\\
    &\vc{x}_{t+1}= 2\vc{x}_{t} -\vc{x}_{t-1}+\frac{\Delta t^2}{m}\vc{f}_t+\Delta t^2\vc{g}\\
    &\vc{x}_0=\vc{a}\\
    &\vc{x}_N=\vc{b}
  \end{array}
  \end{displaymath}

It is not too difficult to extend the above derivations to control a human character. We will need to change the control force $\vc{f}(t)$ to joint torques $\boldsymbol{\tau}(t)$, the physical constraint to the dynamics equation of articulated rigid bodies (eq. \ref{} or \ref{}), and the cost function to the relevant function specific to the task.

There are different options to solve the optimization according to the structure of the problem. Assuming the cost function and the dynamics equations are smooth, Witkin and Kass \cite{} applied a generic nonlinear optimizer, Sequential Quadratic Program (SQP) \cite{}, to solve the problem. It is an iterative optimization technique that solves a sequence of quadratic programs that approximate the original problem. The solution of SQP is an optimal trajectory of state $\vc{x}^*(t)$ and control $\vc{u}^*(t)$, among which $\vc{x}^*(t)$ is the animation of the character for the specified task. Note that this method produces only a \emph{feedforward} (open-loop) controller: a single trajecotry over time. It cannot be generalized to the neighboring regions of the state space. As a result, the controller will fail the task even with a slight disturbance to the motion.

\ignorethis{

  In certain applications of animation, using feedforward controller alone is perfectly fine. However, to combat numerical drift or to recover from unexpected perturbations, a more robust \emph{feedback} (close-loop) controller is needed. It computes the joint torques based on the current states. Most of the motions of real humans require both feedforward and feedback control. For example, our walking gait roughly follows a predefined trajectory (feedforward) but we constantly monitor and control our balance (feedback) based on the road condition, the walking speed and other possible perturbations.
  
A simple way to incorporate feedback is to use Proportional-Derivative (PD) control to follow the optimal state trajectory$\vc{\bar{q}}(t), \vc{\dot{q}}(t)$:
\begin{displaymath}
  \boldsymbol{\tau}=-\vc{K_p}(\vc{q}-\vc{\bar{q}})-\vc{K_d}\vc{\dot{q}}
\end{displaymath}
where $\vc{K_p}$ and $\vc{K_d}$ are the proportional and derivative gains. It applies a spring-like force to correct the deviation from the desired joint trajectory. However, it requires careful manual gain tuning to make it properly work. Small gains result in sloppy tracking while large gains introduce numerical stability issues. Tan et al. \cite{} introduced Stable PD controllers that can greatly improve the numerical stability and alleviate the tedious tunning process.
}

When the cost function is quadratic and the dynamics equation is linear,
\begin{equation}
  \begin{array}{cl}
    \min_{x,u}&\sum_{t=0}^{N}\vc{x}_t^T\vc{Q}_t\vc{x}_t+\vc{u}_t^T\vc{R}_t\vc{u}_t\\
    \textrm{subject to}&\\
    &\vc{x}_{t+1}=\vc{A}_t\vc{x}_t+\vc{B}_t\vc{u}_t
  \end{array}
  \label{eq:LQR}
\end{equation}
the trajectory optimization is called an LQ problem. This problem can be solved very efficiently by linear-quadratic regulator (LQR). The derivation of LQR can be found in most of optimal control textbooks \cite{}, which we will not repeat here. The solution is a feedback controller $\vc{u}_t=\vc{K}_t\vc{x}_t$. Although the requirement of linear dynamics seems quite restrictive, LQR still plays an important role in physically based character animation. One important application is to design a physially-based controller to track motion captured data, which is an effective way to increase the realism of the synthesized motions. Given a motion capture sequence $\vc{\bar{x}}$, we can linearize the dynamics equation at its vicinity:
\begin{displaymath}
  \Delta \vc{x}_{t+1}=\frac{\partial \vc{h}}{\partial \vc{x}}\Delta \vc{x}_{t}+\vc{B}_t\vc{u}_t
  \end{displaymath}
where $\Delta \vc{x}=\vc{x}-\vc{\bar{x}}$. This gives an LQ problem that seek a ``lazy'' (torque minimization) controller $\vc{u}_t=\vc{K}_t\Delta \vc{x}_t$ that minimizes the difference between the actual and the reference motion over the entire trajectory.

More importantly, LQR is a building block to solve the more general trajectory optimization (\ref{eq:trajectoryOptimization}). Given an initial trajectory $\vc{x}_0, \vc{u}_0, \vc{x}_1, \vc{u}_1, ... , \vc{u}_N, \vc{x}_N$, we can perform the following steps iteratively:
\begin{enumerate}
\item{Compute the LQ approximation of the original problem (\ref{eq:trajectoryOptimization}) around the current trajectory by computing a first-order Taylor expansion of the dynamics, and a second-order expansion of the cost function.}
\item{Use LQR to solve the optimal controller $\vc{u}$.}
\item{Generate a new trajectory using controller $\vc{u}$.}
  \item{Go to Step 1 until convergence.}
\end{enumerate}
This iterative-LQR process is the core idea behind differential dynamic programming (DDQ). We refer the interested reader to \cite{} for a more complete discussion about LQR and DDP. The key advantage of DDP is that it not only provides a single trajectory, but also an optimal feedback controller near that trajectory.

To sum up, trajectory optimization is an effective way to synthesize character animation. The synthesized motion is not only physically correct, but more importantly, demonstrates an important animation principle, anticipation. This is because the objective of trajectory optimization is to minimize a long-term cost. Thus, the character can move intelligently that gives up short-term gain to minimize long-term cost. For example, for a high-jump task, the optimal controller could first lower the character's COM to achieve more height later. However, there are a few shortcomings of trajectory optimization. First, it often leads to a high dimensional optimization that is computational very expensive to solve. Another problem of high dimensional optimization is that the nonlinear solver is more likely to get stuck at bad local minima. Thus, good initialization is extremely important. Trajectory optimization utilizes the explicit form of dynamic equations to design the optimal controller. If the dynamic not smooth, too complicated or unknown, it is not clear how to apply the commonly-used trajectory optimization methods.


\subsection{Reinforcement Learning}

Reinforcement learning is motivated by the learning/training process of humans and animals. In a high level, reinforcement learning optimizes a controller by interacting with the physical environment with countless trials. If the controller produces a desired behavior to achieve a task, a reward is provided as the positive reinforcement. In contrast, if an undesired behavior is observed, no reward is offered. This reward system can gradually adapt the controller until eventually, optimal controllers for the task emerge. Reinforcement learning is an active research area, with numerous algorithms proposed every year. We refer readers to \cite{} for a thorough review. We will focus on policy search in remaining of this section. Policy search is a popular reinforcement learning algorithm in physically-based character animation. It performs extremely well in this field because it can tackle control problems with high-dimensional continuous state and action spaces, which is essential to control a human character. 

Mathematically, reinforcement learning formulates and solves a Markov Decision Process (MDP). An MDP is a tuple $(S, A, P_{sas'}, R, D, \gamma)$, where $S$ is the \emph{state} space; $A$ is the \emph{action} space; $P_{sas'}$ is the \emph{transition probability}; $R$ is the \emph{reward function}; $D$ is the distribution of the initial state $s_0 \in D$ and $\gamma \in [0, 1]$ is the discount factor of the reward over time. 

The states reflect what the current situation is and the actions are what a character can perform to achieve the specified task. Ideally, the state space should contain all the possible states of the articulated rigid body system, such as the joint angles $\vc{q}$, joint velocities $\dot{\vc{q}}$ and time $t$, and the actions should consist of all the joint torques $\vc{\tau}$. However, this means that the state and the action space can have hundreds of dimensions. Due to the \emph{curse of dimensionality}, solving an MDP in such a high dimensional continuous space is computational infeasible. In practice, researchers often carefully design states and actions specifically for the tasksin question to make the computation tractable. For example, if the task is to keep balance while standing. The state space need only include important features for balance, such as the center of mass (COM) and the center of the ground contact polygon. Similarly, given the well known balance strategy, such as ankel strategy \cite{} and hip strategy \cite{}, the action space can be as simple as a few torques at lower body joints. Using these prior knowledge of the task can greatly simplify the problem, which is a common practice in physically-based character animation. Transition probability $P_{sas'}$ output the probability that the next state is $s'$ if an action $a$ is taken at the current state $s$. In physically-based character animation, the transition probability is based dynamics. Given $s$ and $a$, a physical simulation can compute the next state $s'$. Although most of the physical simulations output a deterministic state instead of the probability. Random noise can be added in simulation to increase the robustness of the learned controller \cite{}.

A reward function evaluates how good the current state and action are. It is a mapping from the state-action space to a real number: $R: S\times A\mapsto \mathbb{R}$. Similar to Space-time constraints and optimal control, a reward function usually consists of two part, a task-related component that measures how far the current state is from the goal state and a component evaluates the naturalness of the motions. Designing a good reward function is essential to the success of the entire learning algorithm. A good design of the reward should be a smooth function that gives continuous positive reinforce whenever progress is made. Mathematically, this design will present gradient information that optimization solver can follow to maximize the rewards. In contrast, a common mistake is to give reward only when the task is achieved, which makes the reward function a narrow spike and flatly zero elsewhere. This should be avoided because most of the optimization algorithms would have trouble finding such a spike. For certain challenging tasks. manually designing a good reward function is tedious. Another option to extract a reward function from motion capture data through inverse Reinforcement learning \cite{}.

The solution of an MDP is a control \emph{policy} $\pi$ that maps the states space to the action space $\pi: S\mapsto A$. It determines what action to perform at each situation to fulfill the task. Note that the goal of MDP is not to maximize the short-term reward $R$ at the next state, but a long term value $V$. The \emph{return} of a policy is the accumulated rewards along the state trajectory starting at $s_0$ by following the policy $\pi$ for $N$ steps.
\begin{displaymath}
V^\pi(s_0)=\sum_{i=0}^N{\gamma^{N-i}R(s_i)}
\end{displaymath}
The reward at earlier states can be exponentially discounted over time. The \emph{value} of a policy is the expected return with respect to the random initial state $s_0$ drawn from D.
\begin{equation}
V(\pi)=E_{s_0\sim D}[V^\pi(s_0)]
\label{eq:policyValue}
\end{equation}
Using $V$ instead of $R$ as the optimization target prevents the controller from applying short-sighted greedy strategies. This agrees with our ability of long-term planning when we are executing our motions.

A policy can be an arbitrary function. A practical way to optimize the policy is to first parameterize it and then search for the optimal policy within a parameterized functional space $\pi^*\in\Pi$. Commonly-used parameterization include look-up tables, linear functions, splines and neural networks. Parameterization determines the potential quality of the optimal policy. The choice of parameterization is case by case. There is no consensus what the best practice is in terms of policy parameterization. Once the policy parameterization is decided, its parameters are optimized. First, one or more random policies are generated as an initial guess. These candidates are evaluated and improved iteratively until the optimization converges or a user-specified maximum number of iterations is reached.

To evaluate a policy, we can execute the policy in the simulation for N time steps, calculate the reward at each step and sum them up to get the value of the policy. Policy improvement is usually guided using the policy gradient \cite{Ng:2000:PPS}. However, reinforcement learning in character animation faces many unique challenges. In many character animation tasks, such as locomotion and hand manipulation, discrete contact events happens frequently due to establishing and breaking contact, which invalidates gradient and makes it difficult to apply continuous optimization solvers. For this reason, sample-based stochastic optimization techniques are widely used in physically-based character animation. Covariance Matrix Adaptation Evolution Strategy (CMA-ES) \cite{}, is the most ffrequent applied optimization methods for motion control. CMA can work as long as we can evaluate the value function. It does not need to evaluate gradient, does not rely on good initializations. More importantly, CMA is a ``global'' search algorithm that can explore multiple local minima. Although there is no guarantee that CMA will converge at the global minimum, in practice, it often finds good local minima in moderately high dimensional control spaces (\eg 20-30 dimensions). For the completeness of the chapter, we will briefly describe the CMA algorithm. Interested readers can refer to the original paper \cite{} for additional details.

CMA starts with an initial underlying Gaussian distribution in the policy parameter space. A population of samples are generated according to the distribution. Each CMA sample represents a control policy, which is evaluated through simulation. The CMA samples are sorted according to their values and a certain percent of the inferior samples are discarded at each iteration. The underlying Gaussian distribution is updated according to the remaining good samples and is used to generate the next generation of samples. This process is performed iteratively. Over iterations, the underlying distribution is shifted and narrowed. Eventually,  the distribution converges to a good region of the control space. The optimal controller is the  best CMA sample throughout the all iterations.

In summary, reinforcement learning is a generic method for motion control. It can automatically learn a wide range of behaviors through throusands of simulation trials. Reinforcement learning, more specifically, policy search, is becoming an increasingly popular approach to synthesize animations of the virtual characters. One good thing about reinforcement learning is that it does not assume the mathetmaical forms of the dynamics or the reward function. It treats the physical simulation as a black box, as long as the it can output the next state given the current state and action. Thus, it is not bounded to a particular dynamics model. The learning algorithm can be used even if the simulation software is upgraded. The main challenge of reinforcement learning is to design the state, the action space and the reward function. We need to put enough prior knowledge into the design so that the search space is small enough to be computationally feasible, but not too small that eliminates effective control policies. This requires a lot of manual tweaking and experience to make the algorithm work, especially for challenging motion tasks. 
