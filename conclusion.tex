\section{Conclusion and Future Directions}
\subsection{Conclusion}
In this dissertation, we have presented a principled way to synthesize locomotion of humans and animals. Our algorithms can control characters of different morphologies to move efficiently and robustly in complex physically-simulated environments and to achieve challenging tasks. The key components of our algorithms are a set of powerful computational tools, including physical simulation and controller optimization. Although combining simulation and optimization is not a novel idea in motion synthesis, in contrast to prior work, we examine and identify those commonly-used simplifications that can affect the quality of the motions. We eliminate these simplifications by designing new simulation and optimization techniques. Our simulators are faster, more stable and more accurate. Our optimizator can search a higher-dimensional space with both continuous and discrete variables, which may lead to better optimal solutions. These computational tools make it possible to study a more diverse set of motions in nature than were previous possible in the character animation literature.

Chapter 3 described computational tools to study the diversity of swimming motions for aquatic creatures with different body shapes. This is made possible by an accurate swimming simulation and a powerful evolutionary optimization. Compared to the simplified fluid model, our swimming simulation solves the Navier-Stokes equations. It can capture important features of water, including incompressibility and vortices, that affect swimming strategies. In contrast to the traditional alternating two-way coupling technique, our simulation solves the dynamic equations of fluids and articulated rigid bodies simultaneously. This increases the numerical stability and drastically speed up the computation. Simulating the hydrodynamic environment with Navier-Stokes equations introduce new challenges to the classical optimization algorithms. We demonstrated that CMA works well in this scenario. As a result, our algorithms can discover the most efficient swimming gait for a given creature automatically, without any human intervention. Our results showed that the synthesized swimming motions agree well with those employed by real aquatic animals.

Chapter 4 presented computational tools to study locomotion of soft body characters without skeleton support. We developed a muscle model that worked seamlessly with the state-of-the-art FEM simulation for soft materials. This muscle model is inspired by muscle structures found in real soft body animals. It lowers the dimensionality of the control space and ensures that the overall motion is coordinated. We demonstrated how to use finite-horizon trajectory optimization to control the locomotion. The key to our success is to identify that the widely-used simplification that separates contact planning with controller optimization is not good enough to achieve a stable locomotion. To solve this problem, we formulated a QPCC and developed an efficient solver. Consequently, effective control strategies and natural locomotion emerge automatically from the optimization solution.

Chapter 5 demonstrated computational tools to study agile human motions on a bicycle. We developed the first reinforcement learning algorithm that allows a virtual human character to learn bicycle stunts in a physically simulated environment. The algorithm is so efficient that most of stunt actions are learned in hours, which is even faster than the best human stunt bikers. An important lesson we have learned in this work is that it is difficult to design a good controller parametrization manually, especially for challenging locomotion tasks. The common practice of using a fixed policy parametrization tuned by users can severely limit the power of policy search algorithms. We eliminated this restriction by using NEAT, an algorithm that can simultaneously optimize both the parametrization and the parameters of a neural network. Eventually, the virtual character learned to perform a wide variety of stunts automatically, without the tedious manual tuning of controller parametrizations.

Chapter 6 explored an efficient method to develop humanoid robot controllers for the tasks of rising from a leaning/sitting/kneeling position to an erect stance. We built an accurate physical simulation, optimized controllers in the simulation and transferred the controller to a real robot. We investigated several factors that lead to the Reality Gap and demonstrated in several cases that this gap may be crossed with an improved physical simulation. We perform iterative simulation calibration using data collected from robot experiments. After a small number of iterations, the controller designed in a simulation can be successfully transferred to the robot. This work shows that it is possible to apply the computational tools that were developed for character animations to design robotic controllers. This is an important milestone towards a fully automatic computational framework that can design the next-generation robots with extensive agility and manoeuvrability.

\subsection{Future Work}

The research on physically-based character animation has achieved impressive results over the last two decades. However, we are still nowhere near to fully understand the underlying principles of our motions. Many interesting and challenging research still remains. Here I list a few promising future research directions.

\subsubsection{Improving Realism}

One issue of the physically-based character animation is that the synthesized motion is not yet realistic enough for broader applications. It is currently not comparable to the quality of the animations that are hand-tuned by artists. One important reason is that the articulated rigid-body system that are widely used today is a vast simplification of the real human model. A real human has 206 bones and over 600 muscles, which has far more degrees of freedom. In addition, the joints of the articulated rigid body are controlled independently, but human joints move in coordination due to the intricate arrangement of muscles and tendons. A recent trend is to build more sophisticated human models based on biological musculotendon structures \cite{}. It has been demonstrated that an accurate human model can drastically improve the realism of the synthesized motions. As more computational power is made available to us in the next decade, I expect that we will soon be able to afford to use highly detailed human models to synthesize character animation with high fidelity.

Another source of the unnaturalness is due to the hand-crafted objective function in motion control. The objective functions focus mostly on energy efficiency aspect of motions. However, efficient motions do not equal natural motions. Although minimizing energy expenditure is one important factor that govens our motion, it is not the only factor. Our motions are also governed by personal habits, emotion, the task, the environment and many other external factors. It is extremely challenging to hand-craft objective functions for all of them. Assuming that we have abundant of motion data, which is a realistic assumption given the large volume of motion sensors installed in phones and other wearable computing devices, it is promising to extract objective functions from these data using inverse reinforcement learning \cite{}.


\subsubsection{Reducing Prior Knowledge}

Although physically-based character animation frees us from much manual work of traditional animation pipelines, it still requires some high-level prior knowledge to work effectively. For example, we know that regulating the COM of a character relative to the contact points is important for balance tasks. We can inject this prior knowledge by manually choosing the COM and the ground contact points as \emph{features} and include them into the state space of reinforcement learning. Selecting the right features (prior knowledge) is crucial for many of the current control algorithms. However, good features for one task may not carry over to different tasks. Manually selecting features would not scale to more sophisticated characters, more complicated environments, or more challenging tasks. We need an algorithm that can discover control strategies with less or even no prior knowledge. This reminds me of the recent success of deep learning. The way that we are using these hand-engineered features in reinforcement learning today is analogue to using HoG or SIFT features in computer vision a few years ago. Recent advance in computer vision has demonstrated that deep neural networks, such as autoencoder \cite{Vincent:2008} or Restricted Boltzmann machine \cite{Hinton:2012}, can learn features automatically. I believe that the next breakthough in reinforcement learning is to employ similar techniques to automatically discover important features for different motion tasks.

\subsubsection{Bringing the Character to the Real World}

The recent development in physically-based character animation has introduced a set of powerful computational tools. With these tools, natural, agile and robust motions can be synthesized efficiently and autonomously. However, creating lifelike robots is still an extremely challenging, trial-and-error process that is restricted to experts. The fast evolution of 3D printing technology will soon trigger a shift in the robotics industry from mass production to personalized design and fabrication, which will result in an immediate need for a faster, cheaper and more intuitive way to design robotic controllers. The computational tools that are developed in physically-based character animation can potentially automate and streamline the process if we can transfer the controllers from the virtual simulation to the real world. 

Transferring controllers optimized in a simulation onto a real robot is a non-trivial task. An optimal controller that works in a state-of-the-art simulation often fails in a real environment. This is known as the \emph{Reality Gap}. This gap is caused by various simplifications in the simulation, including inaccurate physical model, unmodeled actuator dynamics, assumptions of perfect sensing and zero latency. To fully tap into the power of the computational tools, we need to develop more accurate physical simulations to shrink the the Reality Gap. We probably also need additional models to account for the residual error between the simulation and the dynamics in the real world. A few work \cite{} in physically-based character animation has started to tackle this problem. I believe that with further research and development,  the Reality Gap will shrink rapidly, which will make it much easier to transfer controllers from the simulation to the real world. As a result, I envision that the two separate research fields of character animation and robotics will eventually merge and the computational tools will be shared in both fields. This will inevitably trigger a fundamental revolution in both character animation and robotics.


