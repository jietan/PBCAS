\section{Future Directions}

The research on physically-based character animation has achieved impressive results over the last two decades. However, we are still nowhere near to fully understand the underlying principles of our motions. Many interesting and challenging research still remains. Here I list a few promising future research directions.

\subsection{Improving Realism}

One issue of the physically-based character animation is that the synthesized motion is not yet realistic enough for broader applications. It is currently not comparable to the quality of the animations that are hand-tuned by artists. One important reason is that the articulated rigid-body system that are widely used today is a vast simplification of the real human model. A real human has 206 bones and over 600 muscles, which has far more degrees of freedom. In addition, the joints of the articulated rigid body are controlled independently, but human joints move in coordination due to the intricate arrangement of muscles and tendons. A recent trend is to build more sophisticated human models based on biological musculotendon structures \cite{}. It has been demonstrated that an accurate human model can drastically improve the realism of the synthesized motions. As more computational power is made available to us in the next decade, I expect that we will soon be able to afford to use highly detailed human models to synthesize character animation with high fidelity.

Another source of the unnaturalness is due to the hand-crafted objective function in motion control. The objective functions focus mostly on energy efficiency aspect of motions. However, efficient motions do not equal natural motions. Although minimizing energy expenditure is one important factor that govens our motion, it is not the only factor. Our motions are also governed by personal habits, emotion, the task, the environment and many other external factors. It is extremely challenging to hand-craft objective functions for all of them. Assuming that we have abundant of motion data, which is a realistic assumption given the large volume of motion sensors installed in phones and other wearable computing devices, it is promising to extract objective functions from these data using inverse reinforcement learning \cite{}.


\subsection{Reducing Prior Knowledge}

Although physically-based character animation frees us from much manual work of traditional animation pipelines, it still requires some high-level prior knowledge to work effectively. For example, we know that regulating the COM of a character relative to the contact points is important for balance tasks. We can inject this prior knowledge by manually choosing the COM and the ground contact points as \emph{features} and include them into the state space of reinforcement learning. Selecting the right features (prior knowledge) is crucial for many of the current control algorithms. However, good features for one task may not carry over to different tasks. Manually selecting features would not scale to more sophisticated characters, more complicated environments, or more challenging tasks. We need an algorithm that can discover control strategies with less or even no prior knowledge. This reminds me of the recent success of deep learning. The way that we are using these hand-engineered features in reinforcement learning today is analogue to using HoG or SIFT features in computer vision a few years ago. Recent advance in computer vision has demonstrated that deep neural networks, such as autoencoder \cite{Vincent:2008} or Restricted Boltzmann machine \cite{Hinton:2012}, can learn features automatically. I believe that the next breakthough in reinforcement learning is to employ similar techniques to automatically discover important features for different motion tasks.

\subsection{Bringing the Character to the Real World}

The recent development in physically-based character animation has introduced a set of powerful computational tools. With these tools, natural, agile and robust motions can be synthesized efficiently and autonomously. However, creating lifelike robots is still an extremely challenging, trial-and-error process that is restricted to experts. The fast evolution of 3D printing technology will soon trigger a shift in the robotics industry from mass production to personalized design and fabrication, which will result in an immediate need for a faster, cheaper and more intuitive way to design robotic controllers. The computational tools that are developed in physically-based character animation can potentially automate and streamline the process if we can transfer the controllers from the virtual simulation to the real world. 

Transferring controllers optimized in a simulation onto a real robot is a non-trivial task. An optimal controller that works in a state-of-the-art simulation often fails in a real environment. This is known as the \emph{Reality Gap}. This gap is caused by various simplifications in the simulation, including inaccurate physical model, unmodeled actuator dynamics, assumptions of perfect sensing and zero latency. To fully tap into the power of the computational tools, we need to develop more accurate physical simulations to shrink the the Reality Gap. We probably also need additional models to account for the residual error between the simulation and the dynamics in the real world. A few work \cite{} in physically-based character animation has started to tackle this problem. I believe that with further research and development,  the Reality Gap will shrink rapidly, which will make it much easier to transfer controllers from the simulation to the real world. As a result, I envision that the two separate research fields of character animation and robotics will eventually merge and the computational tools will be shared in both fields. This will inevitably trigger a fundamental revolution in both character animation and robotics.


