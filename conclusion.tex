\section{Future Directions}

The research on physically-based character animation has achieved stunning results in past two decades. However, we are still far from the ultimate goal of character animation: a fully automatic system that can synthesize visually realistic motions that are comparable to those of real humans. Many interesting problems remain to be challenging research problems. Here I list a few promising future research directions.

\subsection{Improving Realism}

One of the biggest issues of the physically-based character animation is its quality. The synthesized motions are not yet realistic enough for broader applications. It is currently not comparable to the quality of the animations that are hand-tuned by artists. One important reason is that the articulated rigid-body systems that are widely used today is a vast simplification of the real human model. A real human has 206 bones and over 600 muscles, which has far more degrees of freedom. In addition, the joints of the articulated rigid body are controlled independently, but human joints move in coordination due to the intricate arrangement of muscles and tendons. A recent trend is to build more sophisticated human models based on biological musculotendon structures \cite{Lee:2006,Lee:2009:CBM,Wang:2012}. It has been demonstrated that an accurate human model can dramatically improve the realism of the synthesized motions. As more computational power is made available to us in the next decade, I expect that we can soon afford to use highly detailed human models to synthesize character animation with high fidelity.

Another source of the unnaturalness is due to the hand-crafted objective function in motion control. The objective functions focus mostly on energy efficiency aspect of motions. However, efficient motions do not equal natural motions. Although minimizing energy expenditure is one important factor that governs our motion, it is not the only factor. Our motions are also governed by personal habits, emotion, task, environment and many other external factors. It is extremely challenging to hand-craft objective functions for all of them. Assuming that we have abundant of motion data, which is a realistic assumption given the large volume of motion sensors installed in phones and other wearable computing devices, it is promising to extract objective functions from these data using inverse reinforcement learning\index{inverse reinforcement learning} \cite{Ng:2000}.


\subsection{Reducing Prior Knowledge}

Although physically-based character animation frees us from much manual work of traditional animation pipelines, it still requires some high-level prior knowledge\index{prior knowledge} to work effectively. For example, we know that regulating the COM of a character relative to the contact points is important for balance tasks. We can inject this prior knowledge by manually choosing the COM and the ground contact points as \emph{features} and include them into the state space of reinforcement learning. Selecting the right features (prior knowledge) is crucial for many of the current control algorithms. However, good features for one task may not carry over to different tasks. Manually selecting features would not scale to more sophisticated characters, more complicated environments, or more challenging tasks. We need an algorithm that can discover control strategies with less or even no prior knowledge. This reminds me of the recent success of deep learning\index{deep learning}. The way that we are using hand-engineered features in reinforcement learning today is analogue to using HoG or SIFT features in computer vision a few years ago. Recent advance in computer vision has demonstrated that deep neural networks, such as autoencoder \cite{Vincent:2008} or Restricted Boltzmann machine \cite{Hinton:2012}, can learn features automatically. I believe that the next breakthrough in reinforcement learning is to employ similar techniques to automatically discover important features for different motion tasks.

\subsection{Bringing the Character to the Real World}

The recent development in physically-based character animation has introduced a set of powerful computational tools. With these tools, natural, agile and robust motions can be synthesized efficiently and autonomously in a simulation. However, creating lifelike robots\index{robot} is still an extremely challenging, trial-and-error process that is restricted to experts. The fast evolution of 3D printing technology will soon trigger a shift in the robotics industry from mass production to personalized design and fabrication, which will result in an immediate need for a faster, cheaper and more intuitive way to design robotic controllers. I believe that the computational tools that are developed in physically-based character animation can potentially automate and streamline the process if we can transfer the controllers from the virtual simulation to the real world. 

Transferring controllers optimized in a simulation onto a real robot is a non-trivial task. An optimal controller that works in a state-of-the-art simulation often fails in a real environment. This is known as the \emph{Reality Gap}\index{Reality Gap}. This gap is caused by various simplifications in the simulation, including inaccurate physical models, unmodeled actuator dynamics, assumptions of perfect sensing and zero latency. To fully tap into the power of the computational tools, we need to develop more accurate physical simulations to bridge the Reality Gap. Researchers in physically-based character animation has started to investigate this problem \cite{Bharaj:2015,Megaro:2015}. I believe that with further research and development, the Reality Gap will shrink rapidly, which will make it easier to transfer controllers from the simulation to the real world. As a result, I envision that the two separate research fields of character animation and robotics will eventually start to merge. This will inevitably trigger a fundamental revolution in both character animation and robotics.


